{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laserhockey.hockey_env as h_env\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from sac import SAC_Agent\n",
    "from dsac import DSAC_Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winner': 0, 'reward_closeness_to_puck': -0.20648411393196578, 'reward_touch_puck': 0.0, 'reward_puck_direction': 0.0}\n",
      "win(left_side): 91,loss: 439, tie: 470\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "env = h_env.HockeyEnv()\n",
    "agent = DSAC_Agent(env.observation_space,env.action_space)\n",
    "agent.load_network_states(torch.load(\"DSAC_Easy_easy-e3000-t32-s42-player.pth\", map_location=torch.device('cpu')))\n",
    "#agent = DSAC_Agent(env.observation_space,env.action_space)\n",
    "#agent.load_network_states(torch.load(\"SAC_run_easy-e1500-t32-s42-player.pth\"))\n",
    "opponent = h_env.BasicOpponent(weak=True)\n",
    "\n",
    "loss = 0\n",
    "win = 0\n",
    "tie = 0\n",
    "\n",
    "for run in range(0,1000):\n",
    "    ob, info = env.reset()\n",
    "    ob_opponent = env.obs_agent_two()\n",
    "    for t in range(1,1000):\n",
    "        action = agent.act(ob)\n",
    "        opponent_action = opponent.act(ob_opponent)\n",
    "        #opponent_action = np.zeros_like(opponent_action)\n",
    "        ob_new,reward,done,trunc,info = env.step(np.hstack([action,opponent_action]))\n",
    "        info_opponent = env.get_info_agent_two()\n",
    "        #if info[\"reward_touch_puck\"]!=0:\n",
    "        #    print(info)\n",
    "        #    print(\"touched\")\n",
    "        #time.sleep(0.01)\n",
    "        #env.render()\n",
    "        if done:\n",
    "            if env.winner ==1:\n",
    "                win +=1\n",
    "            elif env.winner == -1: \n",
    "                loss +=1\n",
    "            else:\n",
    "                tie +=1\n",
    "            break\n",
    "        #if reward != 0:\n",
    "        #    print(reward)\n",
    "        ob = ob_new\n",
    "        ob_opponent = env.obs_agent_two()\n",
    "print(info)\n",
    "env.close()\n",
    "\n",
    "print(f\"win(left_side): {win},loss: {loss}, tie: {tie}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_sac_agent_in_env_modes(agent,mode,log_interval,save_interval,max_episodes,\n",
    "                                 max_timesteps,train_iter,random_seed,name=\"\"):\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    if mode == \"Defense\":\n",
    "        env = h_env.HockeyEnv(mode=h_env.HockeyEnv.TRAIN_DEFENSE)\n",
    "    elif mode == \"Attack\":\n",
    "        env = h_env.HockeyEnv(mode=h_env.HockeyEnv.TRAIN_SHOOTING)\n",
    "    \n",
    "for run in range(0,1):\n",
    "    ob, info = env.reset()\n",
    "    ob_opponent = env.obs_agent_two()\n",
    "    for t in range(1,1000):\n",
    "        action = opponent.act(ob)\n",
    "        opponent_action = agent.act(ob_opponent)\n",
    "        #opponent_action = np.zeros_like(opponent_action)\n",
    "        ob_new,reward,done,trunc,info = env.step(np.hstack([action,opponent_action]))\n",
    "        rewards += [reward]\n",
    "        #if info[\"reward_touch_puck\"]!=0:\n",
    "        #    print(info)\n",
    "        #    print(\"touched\")\n",
    "        #time.sleep(0.01)\n",
    "        #env.render()\n",
    "        if done:\n",
    "            if env.winner ==1:\n",
    "                win +=1\n",
    "            elif env.winner == -1: \n",
    "                loss +=1\n",
    "            else:\n",
    "                tie +=1\n",
    "            break\n",
    "        #if reward != 0:\n",
    "        #    print(reward)\n",
    "        ob = ob_new\n",
    "        ob_opponent = env.obs_agent_two()\n",
    "\n",
    "print(info)\n",
    "print(rewards)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open(f\"SAC_Easy_easy-s42-e4500-stat.pkl\", 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    rewards = np.asarray(data[\"rewards\"])\n",
    "    q_losses =  np.asarray(data[\"q_losses\"])\n",
    "    pi_losses  = np.asarray(data[\"pi_losses\"])\n",
    "    temperature_losses = np.asarray(data[\"temperature_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
